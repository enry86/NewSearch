\documentclass{acm_proc_article-sp-sigmod07}

\usepackage{listings}
\lstset{
language=XML,
basicstyle=\footnotesize,
numbers=left,
numberstyle=\footnotesize,
breaklines=true,
breakatwhitespace=false
}

\begin{document}

\title{Research Project in Data, Media and Knowledge}
\numberofauthors{1}
\author{Enrico Sartori}

\maketitle

\begin{abstract}
Most of the current search engines for documents and news on the web, are
powered by keyword-based indexing systems. The main limit of similar
techniques is that they often cannot detect similarities and relationships
between documents, because they are not expressed at the keyword level.
The aim of this paper is to propose a novel approach to document indexing
and retrieval, able to recognize the actual topic treated in a document
and infer relationships between documents, which could not be detected by
looking just at the keywords.

The problem we are addressing is to build a hierarchy of concepts, which
gives the possibility to infer and discover relationships between articles
and use them to build a search engine for news and articles.
\end{abstract}

\section{APPLICATION}
The setting which we find more representative of the usage of this system
is as the heart of a search engine for news. In our vision it provides the
possibility to build an application used by an advanced category of users,
like students or researchers, to find news and articles belonging to a
specific topic, and related each other.

We refer to a specific target of audience because the system supports,
other than simple keyword-based queries, even semi structured queries.
This could make the use of the search engine less intuitive for a non
technical user.

Moreover, the representation of the output, which is not a flat list of
documents but we intended it as a network of articles reflecting the
existing relationship between documents, is a powerful representation of
the results, but can mislead unexperienced users.

The main focus of the system is to retrieve news from different sources,
in this environment is particularly difficult to state relationships
between documents. In fact many informations aren't explicitly stated in
the text, but, for brevity reason, most of the background is omitted and
left to the reader knowledge.
Therefore in a similar setting the problem of finding and exploiting
hidden relationships between article becomes particularly relevant in
order to retrieve the documents the user is searching for.

\section{MOTIVATION}
The main problems encountered by keyword-based search engines fall in
different fields. First of all, the most common problem related to keyword
indexing is that often the same concept or entity is express using totally
different words.
A simple example of this situation can be given by the expressions ``Barack
Obama'' and ``President of the USA'' which clearly refers to the same
entity but they don't share any word.

A second, more deep, problem which afflicts search engines working just
with keywords is that a generic topic is characterized by many
different aspects. Many articles and documents belongs to the same broad
topic possibly without naming any feature that a similar system could use
to correctly categorize them.
The network of the documents belonging to the same topic and the
relationships between each other shapes a structure which cannot be
detected by looking only at the keywords.
An example of this can be given by an article speaking about a bomb
explosion in Baghdad, which is a news clearly related to the American
foreign policy. Probably the article won't contain any word which
explicitly refers to this topic, because it's assumed that the reader
already knows this kind of relationship.

A third kind of problem that affects common search engine is that they
fail to detect and exploit dependencies between events and topics.
Considering only the keywords the system have no hints that an event can
be related to a specific kind of content.

A last point that we see as a motivation of our work regards the way in
which output is presented to the user. Most search engines nowadays
provide to the user a flat list of results, ranked with respect to some
relevance metric. The aim of this approach is to give more visibility to
the documents that the system retrieved as more strictly related to the
query entered. 
A similar structure doesn't match with the idea of the inferred network of
documents on which our system grounds. We consider an output model that
can explicitly show this network and illustrate the relationships between
each node.

\section{OUR SOLUTION}
This section explains in the details our solution to the problems
discussed before, it covers the all the different aspects of a search
engine: the specification and meaning of the queries, the representation
of the documents, their indexing and retrieval, and the description of the
output model.

\subsection{Query}
In order to reach a compromise between usability of the system and
expressiveness of the query, our platform supports different category of
queries. As many common search engines it accepts queries expressed
as a list of keywords, this is the most simple approach and permits a very
intuitive and easy interaction with the system.

A more formal representation of this query model can be expressed as:
$$
Q_{keywords} = \{w_1,\dots,w_n\}
$$
Which is basically a list of the n keywords inserted by the user.

A slightly more complex kind of query supported by the system follows the
semi-structured model. This kind of interaction, besides being more
complex and less intuitive than the previous, increases the expressive
power of the query.
In fact it's possible to submit queries in the form ``City = London, Name
= Jack'', which, intuitively, carries much more information than the
equivalent ``London Jack'' as it would be expressed in the previous model.

The formalization of this particular query model can be seen as a list of
pairs (key, value).
$$
Q_{semi-structured} = \{<k_1, v_1>, \dots, <k_n, v_n>\}
$$

\subsection{Entities}
One of the fundamental building block of our system is represented by the
entities. We consider an entity any kind of object, like a public person,
an enterprise, an event, which can be uniquely identified among all other
objects.
A concrete example of what we consider as an entity can be the city of
Rome, which will have its own unique id. A different way to refer to Rome
can be ``capital of Italy'' but it's just a different way to name the very
same entity.

The use of this level of abstraction permits to overcome the first problem
we pointed out in the motivation section. In fact all the different way to
refer to the same entity are grouped together and treated as a single
object.

Formally, inside the system, an entity is seen as an unique id, with
associated a list of vector of keywords. Each vector contains the keywords
used to refer to that entity.
A formulation of the above concept is the following:
$$
E = <id, [\{w_{11}, \dots, w_{1m}\},\dots, \{w_{k1}, \dots, w_{kn}\}]>
$$

In order to retrieve the entities appearing in a document, we had to rely
on existing services. The service we used for our system is OpenCalais,
which permits programmatic access via a SOAP web-service.

\paragraph*{OpenCalais}
The scope of the OpenCalais project is to provide a service for locating
Named Entities within a document. As we ground the representation of the
documents in terms of this kind of objects, we can take advantage of
similar services to perform the entity recognition task.

The choice of OpenCalais in our implementation has been guided by its
renown inside the semantic web community, and by the quality of the
service offered.

The main method exported by this platform is the following:
\begin{verbatim}
Enlighten(key, content, configuration)
\end{verbatim}
The most important argument of this function is content, which is the
actual content of the article analyzed. The value returned by this call is
a file containing the entities inside the text.

The service can handle different output formats like XML, JSON, or
Microformats. 

In particular the service isolates the sentences surrounding the
reference to the entity. This can be useful to find occurrences of
keywords related to this object.


\subsection{Document representation}
As pointed out before, the main object with which the system deals are the
entities appearing in the corpus of documents analyzed. Therefore the
representation of the document has the function to make possible, once
selected the set of entities related to the query submitted, to retrieve
the documents related to that particular set of entities.

We considered a useful choice for a document representation in this
setting, to have the text of the article, associated with the set of
entities retrieved.
The OpenCalais system provides a measure of the relevance of an entity
inside the document.

Another important feature that the system retrieves from the text are
relationships existing between entities. The next section will give a
definition of relationship for our system.

Putting all the informations together we can define formally a document
as:
$$
D = <text, \{<e_{1},r_{1}>, \dots, <e_{n}, r_{n}>\}, R>
\label{eq:doc}
$$

In the above formula $R$ is a representation of the relationships
existing between the entities in the document.


\subsection{Relationship among entities}
Once the system has analyzed a of document, and the entities
appearing into it have been defined, is necessary to find relationship
between different entities. This step permits to reconstruct the structure
of links between entities inside the document, existing at a
higher level of abstraction.

What we consider a relationship insisting among a group of entities can be
represented by the actions which those entities are performing together.
In the natural language text, those actions are expressed in form of
verbs. Our approach intend to isolate sentences in the text and retrieve
the verbs which are linking together different entities.

Therefore we can define the set $R$ appearing in formula \ref{eq:doc}
formally as:
$$
R = \{ <\{e_{i}, \dots, e_{k}\}, \{v_{n}, \dots, v_{m}\}>, \dots \}
$$


From this we can define the explicit relationship between two of entities as a
vector of verbs as stated in the following:
$$
C_{e}(e_{i}, e_{j}) = \{v_{1}, \dots v_{n}\}
$$

The network of relationships which results from this analysis can be
represented as a graph, in which each node is a set of entities and the
values on the arches are the verbs connecting the different sets of
entities.

This approach gives a higher level view on the meaning of the single
keywords in the document, based on an effective interpretation of natural
language sentences.

\subsection{Ghost entities}
Once we obtained the graph of relationships existing among entities
appearing in a document, we can infer further relationships, exploiting
the similarities between graphs representing other documents.

The term ``Ghost Entity'' represent in fact an entity which is not stated
explicitly in the document but can be added because appears in many other
documents as related to an entity appearing in the document considered.

The strength of the relationship with other entities in the document is
computed considering different factors.
We consider the distance in the graph between the entities, the number of
documents in which the two entities appears as related weighted over a
measure of similarity between the two documents.

Formally, the value of the relationship is given by the formula:
$$
r(e_{i}, e_{j}) = \frac{ \sum_{d_{k} \in D_{e_{i}, e_{j}} } {sim(d, d_{k}) } \cdot
dist_{d_{k}}(e_{i},e_{j}) }{|D_{e_{i},e_{j} }|}
$$

Where the function $sim$ is the similarity between two documents, computed
in terms of graph similarity, $dist$ is the distance between two entities
inside a document $d_{k}$.
The set $D_{e_{i}, e_{j}}$ is the set of documents in which the entities
$e_{i}$ and $e_{j}$ appear together explicitly.


\subsection{Query Answering}
Once we have built the graph of entities, which represent our indexing
structure, the system can provide the query answering service.
Answering a query is a process composed of different phases.

First of all we need to analyze the query sent by the user and retrieve
the entities directly correlated to the terms of the query.
This part of the process varies in function of the kind of query
submitted. As long as the system supports keyword based queries or
semi-structured ones, we need different procedures to select the set of
relevant entities.

When we have isolated the set of relevant entities, we need to expand our
selection following the inferred relationship starting form the previously
selected nodes.

Now we need to map the set of entities we consider relevant with the
query, down to the related documents, caring of maintaining the coherence
of the relationships.

The last step is to output the graph of the retrieved documents to the
user, showing the relationship which is bounding the documents to each
other.

\section{Implementation}
This section describes some details of our implementation of the system,
focusing on how we realized the ideas expressed in the previous section.
The implementation covers all the different phases of the system
lifecycle.

In fact the architectural choices behind our implementation lead us to
develop a set of independent tools, each one of them dedicated to perform
a specific task inside the system. 

The motivation behind this choice was to enhance the possibility of
parallelization of the processes, even have different tools running of
different machines.

Moreover a similar architecture can tolerate more easily a crash of the
machine, considering that each tool persists its data on disk is easy to
recover the state of the system without losing the work done.

The different tasks the system has to perform can be summarized by the
following list:
\begin{itemize}
\item Crawling
\item Query OpenCalais
\item Document analysis and representation
\item Index building
\item Query answering
\end{itemize}

\subsection{Crawler}
The tool implementing the crawler (\emph{smith.py}) takes care of
downloading from the web a set of articles taken from a list of online
newspapers.

The approach followed for retrieving the articles is by reading the RSS
feed of the various sources, fetching the new articles as they are
published.

The crawler stores the pages on disk, ready to be used by the other tools
and updates a database of the retrieved pages, in order to avoid
downloading multiple times the same article.

In order to maximize the performances of the crawler is important to
parallelize the requests to the different websites.
The system can be used following two different approaches, it can work
with asynchronous sockets or it can make use of multiple thread. The
default behaviour is to dedicate a thread to each page fetching.

\subsection{OpenCalais client}
As underlined in the previous sections our implementation relies on the
OpenCalais service for the Entity retrieval task. Therefore we developed a
client application to the service (\emph{oc\_soap.py}), which uses the SOAP protocol to
communicate with the OpenCalais Web service. 

A drawback of this approach is obviously the latencies and the high amount
of time spent during the communication with the service. An additional
tool providing this service, hosted on the local machine, would improve
greatly the performances, but, given the quality of the service offered by
OpenCalais, we decided to use it for this test implementation of the
system.

The client sends the raw HTML of a page to the service, which, before
analyzing the document, cleans the page received from the markup tags and
extracts the relevant text.
The results of the document analysis is sent back to the client as an XML
representation of an RDF tree. 

The client requires that the OpenCalais service inserts in the XML
response the actual text used for the analysis. This data will be useful
during the subsequent steps to retrieve additional information from the
document.

Once the XML file is stored on disk, the corresponding HTML file is
deleted, in order to not waste resources keeping useless files.

\subsection{Document analysis}
The task of analyzing the document, along with the representation of the
entities coming from OpenCalais, is pivotal in the lifecycle of the
system. The tool implementing this task computes a representation of the
document in the form of a graph of entities and the relationships between
them.

The tool implemented (\emph{verba.py}) takes care of parsing the XML
coming from OpenCalais, retrieving the information about the entities.
As specified previously the representation given to an entity is an Id
(given by OpenCalais) and the list of words used to represent the entity
in each occurrence in the document.

To perform this task the tool can rely on the information given by
OpenCalais, in fact the RDF explicits the exact sentence containing each
entity.
Parsing the XML file is easy to retrieve the list of keywords describing
the entities.

The second step in the documents analysis is to discover relationships
between entities appearing in the document. The approach used in this
task is to apply Natural Language Processing techniques in order to
retrieve the verbs, and therefore the actions, linking together different
entities.

In our implementation we relied on the NLTK toolkit as a library of tools
useful in performing NLP tasks. What the system does is constructing a
parse tree of each sentence in the document and isolate the verbs
referring to each entity.

The construction of this grammar tree is performed through three phases,
first we need to locate and mark the position of the entities inside the
document, so once the tree is built is possible to recognize the entity
inside of it and retrieve the correct verb.

Given that the computation of the parse tree is a costly operation in
terms of time, in this first step we take care of filter out the sentences
which don't contains entities. In this way we can slightly reduce the
effort needed to process an entire document.

The second phase of the process is called Part Of Speech tagging (POS
tagging) and resolves into a classification of each word in the sentence
with respect to its grammatical role.
The ``tag'' applied to each word permits to distinguish between nouns,
verbs, conjunctions, articles, etc.

This task is performed relying on pre-trained models offered by NLTK,
otherwise is possible to train a classifier over a corpus of tagged
sentences.

Once the words in the sentence are tagged is possible to run a parser over
the list of tags and build the actual tree. NLTK offers the possibility to
build such a parser using a set of parsing rules defined by the
programmer. 

The technique used is to define these rules as regular expressions, which
define the grammatical structure of a sentence in terms of POS tags.
The resulting parsers reads the POS tags list applying the rules whenever
possible, constructing the resulting tree structure.

When the grammar tree of the sentence is built is possible to visit the
tree in order to find the entities and retrieve the verbs which are
referring to them. 
The information gathered in this step is stored in a graph-like data
structure, where the nodes are in fact the entities and the arches between
them are the verbs linking them together.

The graph resulting from the analysis of all the relevant sentences in the
document is the final representation of the document itself in our system,
and can be stored into a database for the subsequent operations.







\end{document}
